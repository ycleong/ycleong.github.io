<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yuan Chang Leong, PhD</title>
    <link>https://ycleong.github.io/</link>
      <atom:link href="https://ycleong.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Yuan Chang Leong, PhD</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 31 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ycleong.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Yuan Chang Leong, PhD</title>
      <link>https://ycleong.github.io/</link>
    </image>
    
    <item>
      <title>Pupil-linked arousal biases evidence accumulation towards desirable percepts during perceptual decision-making</title>
      <link>https://ycleong.github.io/publication/pupil2020/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/pupil2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Motivated visual perception</title>
      <link>https://ycleong.github.io/project/a_visualperception/</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/project/a_visualperception/</guid>
      <description>&lt;p&gt;Individuals with different motivations often report seeing the same image differently, but it has long been disputed whether this reflects a bias in what they &lt;em&gt;see&lt;/em&gt; or what they &lt;em&gt;report&lt;/em&gt; seeing. Can people with diverse goals and motives truly perceive the same visual stimuli differently? I re-examined this decades-old problem using computational modeling and fMRI &lt;a href=&#34;https://www.nature.com/articles/s41562-019-0637-z&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;. I measured the neural responses of participants as they viewed visually ambiguous images created by morphing together a face and a scene. Participants were rewarded for correctly judging whether the image contained ‘more face’ or ‘more scene’. For each image, I motivated them to see one of the categories by instructing them that they would earn a bonus if the image contained more of that category.&lt;/p&gt;
&lt;p&gt;Participants were more likely to report seeing the image as containing more of the category they were motivated to see. If motivation altered perceptual experience, we would expect to observe a corresponding effect on neural representations in visual areas of the brain. Indeed, for a given image, face-related neural activity was higher when participants were motivated to see more face, while scene-related activity was higher when they were motivated to see more scene. Motivationally biased judgments were associated with increased activity in frontoparietal regions that I had previously found to be involved in the dynamic control of attention &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S089662731631039X&#34; target=&#34;_blank&#34;&gt;[2]&lt;/a&gt;, suggesting a potential source of top-down signals driving these effects.&lt;/p&gt;
&lt;p&gt;Analyzing participants’ behavior using a computational model of the underlying decision processes, I further showed that participants’ motivational bias could be decomposed into an &lt;em&gt;a priori&lt;/em&gt; bias to respond in a motivation-consistent manner (i.e. a ‘response’ bias), as well as a bias in the accumulation of sensory information (i.e. a ‘perceptual’ bias). The response bias was associated with anticipatory activity in the nucleus accumbens, a striatal structure implicated in reward processing and action selection. In contrast, the perceptual bias tracked modulations in category-selective neural activity in the visual cortex.&lt;/p&gt;
&lt;p&gt;Reframing the problem as a ‘how’ question opens the door to new lines of enquiry about mechanisms. Drawing on the intuition that motivation tends to be arousing, as well as prior work showing that arousal tunes visual attention, I hypothesized that motivational effects on perception are mediated by changes in physiological arousal. In my postdoctoral work, I show that heightened arousal, as measured using pupillometry, was indeed associated with motivational biases in the accumulation of perceptual information &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2020.05.29.124115v1&#34; target=&#34;_blank&#34;&gt;[3]&lt;/a&gt;. Ongoing and future work further explores this topic by examining the temporal dynamics of motivational biases, the neural mechanisms that mediate arousal effects, and the different effects of approach and avoidance motivations. In bringing together a formal characterization of behavior, affective states and brain function, this work seeks to advance an integrative understanding of motivated visual perception.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Media&lt;/strong&gt;:&lt;br&gt;
&lt;a href=&#34;https://www.vox.com/science-and-health/2019/8/8/20706126/motivated-perception-psychology&#34; target=&#34;_blank&#34;&gt;How desire can warp our view of the world&lt;/a&gt; (Vox, 8.8.2019)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.psychologytoday.com/us/blog/between-cultures/201907/why-we-see-what-we-want-see&#34; target=&#34;_blank&#34;&gt;Why we see what we want to see&lt;/a&gt; (Psychology Today, 7.9.2019)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nature.com/articles/s41562-019-0639-x&#34; target=&#34;_blank&#34;&gt;Is visual representation coloured by desire?&lt;/a&gt; (News and Views, Nature Human Beahvior, 7.1.2019)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Leong, Y. C.&lt;/strong&gt;, Hughes, B. L., Wang, Y., &amp;amp; Zaki, J. &lt;a href=&#34;https://www.nature.com/articles/s41562-019-0637-z&#34; target=&#34;_blank&#34;&gt;Neurocomputational mechanisms underlying motivated seeing&lt;/a&gt;. &lt;em&gt;Nature Human Behaviour&lt;/em&gt;, 3(9): 962-973 (2019)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Leong, Y. C.&lt;code&gt;*&lt;/code&gt;&lt;/strong&gt;, Radulescu, A.&lt;code&gt;*&lt;/code&gt;, Daniel, R., DeWoskin, V., &amp;amp; Niv, Y. &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S089662731631039X&#34; target=&#34;_blank&#34;&gt;Dynamic interaction between reinforcement learning and attention in multidimensional environments&lt;/a&gt;. &lt;em&gt;Neuron&lt;/em&gt;, 93(2): 451-463 (2017)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Leong, Y. C.&lt;/strong&gt;, Dziembaj, R. &amp;amp; D&amp;rsquo;Esposito, M. &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2020.05.29.124115v1&#34; target=&#34;_blank&#34;&gt;Pupil-linked arousal biases evidence accumulation towards desirable percepts during perceptual decision-making&lt;/a&gt;. &lt;em&gt;bioRxiv&lt;/em&gt; (2020)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Motivated political reasoning</title>
      <link>https://ycleong.github.io/project/b_politics/</link>
      <pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/project/b_politics/</guid>
      <description>&lt;p&gt;Biased assimilation of information drives increasing political polarization – the same event, perceived differently by conservatives and liberals, cause both groups to become more entrenched in their beliefs. The consequences of biased perceptions on attitude polarization are well-documented, but it remains unclear how exactly political information is perceived differently. Do the differences emerge as a result of biases in sensory attention, in that people attend more to information supporting their beliefs, or do they reflect differences in interpreting the same sensory input? What type of content drives polarization? I sought to address these questions by combining neuroimaging and semantic analyses of real-world political messages &lt;a href=&#34;https://www.pnas.org/content/early/2020/10/19/2008530117&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I measured the neural response of conservative and liberal participants watching news clips, campaign ads and political speeches related to immigration policy. I then searched the brain for neural responses that diverged between conservatives and liberals. If motivation biased sensory attention, we would expect differences in neural responses to emerge in sensory areas. Alternatively, if motivation biases the interpretation of the same sensory input, the differences would emerge only in higher-order brain areas. Consistent with the latter hypothesis, I found that the neural responses of conservative and liberal participants diverged only in the dorsomedial prefrontal cortex (DMPFC), a brain region previously shown to track the interpretation of narratives. Furthermore, the degree to which a participant’s DMPFC response was similar to that of the average conservative or average liberal participant predicted subsequent attitude change towards conservative or liberal positions respectively, suggesting that adopting an interpretation closer to a particular group biased attitudes towards positions held by that group.&lt;/p&gt;
&lt;p&gt;A challenge faced by prior work is that it is often difficult to precisely assess how the interpretation of two individuals differ based on their verbal reports or behavioral ratings. My approach side-steps this limitation by computing the divergence in DMPFC activity between conservative and liberal participants at each timepoint as a continuous and temporally resolved neural measure of divergent interpretations. The richness of the real-world stimuli used in the study then made it possible to take a data-driven approach to investigate the relationships between message content and biased processing. I broke down the content of the videos into 50 semantic categories and showed that the divergence in DMPFC activity was stronger during moments in the videos with moral-emotional and threat-related language, highlighting the content features most likely to drive divergent interpretations. Future work will combine neuroimaging data with sentence-embedding methods from natural language processing to build more sophisticated and specific semantic models of how political content is interpreted, with the goal of informing interventions aimed at aligning interpretations between conservatives and liberals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Media&lt;/strong&gt;:&lt;br&gt;
&lt;a href=&#34;https://news.berkeley.edu/2020/10/20/hot-button-words-trigger-conservatives-and-liberals-differently/&#34; target=&#34;_blank&#34;&gt;Hot-button words trigger conservatives and liberals differently&lt;/a&gt; (Berkeley News, 10.20.2020)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.brainpost.co/weekly-brainpost/2020/10/27/political-views-bias-information-processing-in-the-brain&#34; target=&#34;_blank&#34;&gt;Political Views Bias Information Processing in the Brain&lt;/a&gt; (BrainPost, 10.27.2020)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.medscape.com/viewarticle/939835&#34; target=&#34;_blank&#34;&gt;Brain Imaging Reveals a Neural Basis for Partisan Politics&lt;/a&gt; (Medscape Medical News, 10.27.2020)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Leong, Y. C.&lt;/strong&gt;, Chen, J., Willer, R., &amp;amp; Zaki, J. &lt;a href=&#34;https://www.pnas.org/content/early/2020/10/19/2008530117&#34; target=&#34;_blank&#34;&gt;Conservative and liberal attitudes drive polarized neural responses to political content&lt;/a&gt;. &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; (in press)&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Motivated social perception</title>
      <link>https://ycleong.github.io/project/c_socialperception/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/project/c_socialperception/</guid>
      <description>&lt;p&gt;People are motivated to seek out socially valued individuals (i.e. people who are trustworthy, supportive and well-connected), but it is unclear how efficiently and accurately they do so in real-world settings. I used a combination of social network analysis and neuroimaging to test the possibility that – even absent explicit instructions – people accurately track the social value of others in their community &lt;a href=&#34;https://www.pnas.org/content/115/32/8149&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;. Socially valued individuals in two dormitories were identified from the nominations of dorm residents. I then scanned the students as they passively viewed photos of their dorm-mates. I found that brain activity in regions associated with mentalizing and value computation differentiated between viewing high valued, versus less valued peers, suggesting that people spontaneously and accurately monitor peers’ social value, potentially to guide subsequent social interactions.&lt;/p&gt;
&lt;p&gt;What happens when expectations of social value are misinformed, for example, when someone is less trustworthy than his or her reputation suggests? Will people quickly adjust their impressions, or will they perseverate on their initial expectations? In a series of behavioral studies, I show that having overly optimistic expectations about an interaction partner blinds participants to the failures of said partner in an advice-taking task, resulting in participants continuing to trust the partner’s advice despite repeated feedback that the advice is inaccurate  &lt;a href=&#34;https://psycnet.apa.org/record/2017-52070-001&#34; target=&#34;_blank&#34;&gt;[2]&lt;/a&gt;. Together, this line of work parallels my other work on visual perception and demonstrates how social value biases representations of the social world.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Morelli, S. A.&lt;code&gt;*&lt;/code&gt;, &lt;strong&gt;Leong, Y. C.&lt;code&gt;*&lt;/code&gt;&lt;/strong&gt;, Carlson, R. W., Kullar, M., &amp;amp; Zaki, J. &lt;a href=&#34;https://www.pnas.org/content/115/32/8149&#34; target=&#34;_blank&#34;&gt;Neural detection of socially valued community members&lt;/a&gt;. &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt;, 115(32): 8149-8154 (2018)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Leong, Y. C.&lt;/strong&gt; &amp;amp; Zaki, J. &lt;a href=&#34;https://psycnet.apa.org/record/2017-52070-001&#34; target=&#34;_blank&#34;&gt;Unrealistic optimism in advice taking: A computational account&lt;/a&gt;. &lt;em&gt;Journal of Experimental Psychology: General&lt;/em&gt;, 147(2): 170 (2018)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Communication and shared experiences</title>
      <link>https://ycleong.github.io/project/d_communication/</link>
      <pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/project/d_communication/</guid>
      <description>&lt;p&gt;A (beautiful) feature of human experience is that we don&amp;rsquo;t have to experience everything ourselves. Through the stories we tell one another, we can, to a degree, experience the life of another.&lt;/p&gt;
&lt;p&gt;In work with collaborators, I have explored the neural underpinnings of how people talk to and understand each other. We scanned participants watching a 50-minute movie clip and talking about the plot immediately afterwards. We show that watching and talking about the same scene elicits a similar pattern of neural activity. The pattern was also shared across people talking about the same scene, revealing the existence of a common &amp;lsquo;neural code&amp;rsquo; that could serve as the foundation for human communication &lt;a href=&#34;https://www.nature.com/articles/nn.4450&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In a second study, we scanned a group of participants who had not watched the movie as they listened to someone talking about the movie. We found that listening to the speaker talk about a scene elicited a pattern of neural activity that was similar to that of the speaker watching that scene, despite participants not having seen the movie themselves &lt;a href=&#34;https://academic.oup.com/cercor/article/27/10/4988/4080827&#34; target=&#34;_blank&#34;&gt;[2]&lt;/a&gt;. Together, these results suggest that communication is associated with the construction and transmission of shared neural representations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Chen, J.&lt;code&gt;*&lt;/code&gt;, &lt;strong&gt;Leong, Y. C.&lt;code&gt;*&lt;/code&gt;&lt;/strong&gt;, Honey, C. J., Yong, C. H., Norman, K. A., &amp;amp; Hasson, U. &lt;a href=&#34;https://www.nature.com/articles/nn.4450&#34; target=&#34;_blank&#34;&gt;Shared memories reveal shared structure in neural activity across individuals&lt;/a&gt;. &lt;em&gt;Nature Neuroscience&lt;/em&gt;, 20(1): 8149-8154 (2017)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zadbood, A., Chen, J., &lt;strong&gt;Leong, Y. C.&lt;/strong&gt;, Norman, K. A., &amp;amp; Hasson, U. &lt;a href=&#34;https://academic.oup.com/cercor/article/27/10/4988/4080827&#34; target=&#34;_blank&#34;&gt;How we transmit memories to other brains: constructing shared neural representations via communication&lt;/a&gt;. &lt;em&gt;Cerebral Cortex&lt;/em&gt;, 27(10): 4988-5000 (2017)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Narratives: fMRI data for evaluating models of naturalistic language comprehension</title>
      <link>https://ycleong.github.io/publication/nastase2020/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/nastase2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Conservative and liberal attitudes drive polarized neural responses to political content</title>
      <link>https://ycleong.github.io/publication/pnas2020/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/pnas2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neurocomputational mechanisms underlying motivated seeing</title>
      <link>https://ycleong.github.io/publication/nhb2019/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/nhb2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unrealistic optimism in advice taking: A computational account</title>
      <link>https://ycleong.github.io/publication/jpeg2019/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/jpeg2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural detection of socially valued community members</title>
      <link>https://ycleong.github.io/publication/pnas2019/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/pnas2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamic interaction between reinforcement learning and attention in multidimensional environments</title>
      <link>https://ycleong.github.io/publication/neuron2017/</link>
      <pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/neuron2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How we transmit memories to other brains: Constructing shared neural representations via communication</title>
      <link>https://ycleong.github.io/publication/cerebcortex2017/</link>
      <pubDate>Thu, 05 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/cerebcortex2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Shared memories reveal shared structure in neural activity across individuals</title>
      <link>https://ycleong.github.io/publication/natneu2017/</link>
      <pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/natneu2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning and making novel predictions about others&#39; preferences</title>
      <link>https://ycleong.github.io/publication/cogsci_2016/</link>
      <pubDate>Tue, 05 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/cogsci_2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reinforcement learning in multidimensional environments relies on attention mechanisms</title>
      <link>https://ycleong.github.io/publication/jneuro2015/</link>
      <pubDate>Wed, 27 May 2015 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/jneuro2015/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human reinforcement learning processes act on learned attentionally-filtered representations of the world</title>
      <link>https://ycleong.github.io/publication/rldm_2013/</link>
      <pubDate>Sat, 05 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/rldm_2013/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On musical dissonance</title>
      <link>https://ycleong.github.io/publication/music2012/</link>
      <pubDate>Sat, 01 Sep 2012 00:00:00 +0000</pubDate>
      <guid>https://ycleong.github.io/publication/music2012/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
